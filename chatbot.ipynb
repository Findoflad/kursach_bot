{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLsNzqZKfs4F5ua/UWew0O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Findoflad/kursach_bot/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.python.keras.optimizers import gradient_descent_v2 \n",
        "import random\n",
        "from keras.models import load_model\n",
        "\n",
        "# загружаем файл для подготовки\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?','!']\n",
        "data_file = open(\"/content/sample_data/Train_Bot.json\").read()\n",
        "intents = json.loads(data_file)\n",
        "\n",
        "# подготавливаем данные для обработки, в частности токенизируем их\n",
        "# и разбиваем на слова, документы, классы\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    words.extend(w)\n",
        "    # добавляем документы к корпусу\n",
        "    documents.append((w, intent['tag']))\n",
        "\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "# теперь лемматизация\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "# применяя сериализацию сохраняем полученные объекты для дальнейшего использования\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))\n",
        "\n",
        "# создаем данные для обучения\n",
        "training = []\n",
        "\n",
        "# объявляем пустой массив для вывода\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# создаем модель мешка слов для каждого предложения\n",
        "for doc in documents:\n",
        "  bag = []\n",
        "  pattern_words = doc[0]\n",
        "  pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# создаем списки для тренировки и тестов\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "\n",
        "# последовательная модель для предсказания ответов\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# компилируем модель\n",
        "sgd = gradient_descent_v2.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "# сохраняем и загружаем для дальнейшего использования\n",
        "#model.save('chatbot.h5', hist)\n",
        "model = load_model('/content/chatbot.h5')\n",
        "\n",
        "# теперь пользовательский ввод\n",
        "def clean_up_sentence(sentence):\n",
        "  # токенизируем предложение\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  # и приводим каждое слово к первоначальной форме\n",
        "  sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "  return sentence_words\n",
        "\n",
        "# модель мешка слов для введенного предложения\n",
        "def bow(sentence, words, show_details=True):\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "\n",
        "  bag = [0]*len(words)\n",
        "  for s in sentence_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w == s:\n",
        "\n",
        "        bag[i] = 1\n",
        "        if show_details:\n",
        "          print(\"found in bag: %s\" % w)\n",
        "  return(np.array(bag))\n",
        "\n",
        "# функция для прогноза ответа\n",
        "def predict_class(sentence, model):\n",
        "  p = bow(sentence, words, show_details=False)\n",
        "  res = model.predict(np.array([p]))[0]\n",
        "  error = 0.25\n",
        "  results = [[i,r] for i,r in enumerate(res) if r>error]\n",
        "\n",
        "  results.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "\n",
        "  for r in results:\n",
        "    return_list.append({\"intent\": classes[r[0]], \"probability:\": str(r[1])})\n",
        "  return return_list\n",
        "\n",
        "# функция получения ответа от модели\n",
        "def getResponse(ints, intents_json):\n",
        "  tag = ints[0]['intent']\n",
        "  list_of_intents = intents_json['intents']\n",
        "  for i in list_of_intents:\n",
        "    if(i['tag']==tag):\n",
        "      result = random.choice(i['responses'])\n",
        "      break\n",
        "  return result\n",
        "\n",
        "# функция для прогноза класса и вывода соответствующего ответа\n",
        "def chatbot_response(text):\n",
        "  ints = predict_class(text, model)\n",
        "  res = getResponse(ints, intents)\n",
        "  return res\n",
        "\n",
        "# запус бота\n",
        "def start_chat():\n",
        "  print(\"Bot: Hello Master. \\n\\n\")\n",
        "  while True:\n",
        "    inp = str(input()).lower()\n",
        "    if inp.lower()==\"end\":\n",
        "      break\n",
        "    if inp.lower()=='' or inp.lower()=='*':\n",
        "      print(\"Please rewrite your phrase\")\n",
        "      print(\"_\"*50)\n",
        "    else:\n",
        "      print(f\"Bot: {chatbot_response(inp)}\"+'\\n')\n",
        "      print(\"_\"*50)\n",
        "\n",
        "start_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMFXu1Gm74Si",
        "outputId": "82f59904-f0ae-4dcc-8300-f60a8f0998a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Bot: Hello Master. \n",
            "\n",
            "\n",
            "end\n"
          ]
        }
      ]
    }
  ]
}